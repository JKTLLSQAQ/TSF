from exp.exp_basic import Exp_Basic
from models import dual, redual, STAR, FusedTimeModel
from data_provider.data_factory import data_provider
from utils.tools import EarlyStopping, adjust_learning_rate
from utils.metrics import metric
import torch
import torch.nn as nn
import torch.nn.functional as F
from torch import optim
import os
import time
import warnings
import numpy as np
import pandas as pd
from torch.optim import lr_scheduler
from tqdm import tqdm
import matplotlib.pyplot as plt
import matplotlib.dates as mdates
from datetime import datetime
import seaborn as sns
from exp.exp_learning_rate import (AdaptiveLossLRScheduler, CombinedLRScheduler, plot_lr_loss_history, )
from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score
import argparse

warnings.filterwarnings('ignore')

class BarronAdaptiveLoss(nn.Module):
    """Barronè‡ªé€‚åº”æŸå¤±å‡½æ•°"""

    def __init__(self, alpha_init=2.0, scale_init=1.0):
        super().__init__()
        self._alpha = nn.Parameter(torch.tensor(alpha_init, dtype=torch.float32))
        self._scale = nn.Parameter(torch.tensor(scale_init, dtype=torch.float32))

    @property
    def alpha(self):
        return F.softplus(self._alpha) + 0.01

    @property
    def scale(self):
        return F.softplus(self._scale) + 1e-6

    def forward(self, pred, target):
        residual = pred - target
        alpha = self.alpha
        scale = self.scale

        normalized_residual = residual / scale

        if torch.abs(alpha - 2.0) < 1e-4:
            loss = 0.5 * normalized_residual.pow(2)
        else:
            abs_alpha_minus_2 = torch.abs(alpha - 2.0)
            inner_term = (normalized_residual.pow(2) / abs_alpha_minus_2 + 1).pow(alpha / 2) - 1
            loss = (abs_alpha_minus_2 / alpha) * inner_term

        return loss.mean()

    def get_params(self):
        return {
            'alpha': self.alpha.item(),
            'scale': self.scale.item()
        }


def generate_timestamp():
    """ç”Ÿæˆæ—¶é—´æˆ³å­—ç¬¦ä¸²"""
    return datetime.now().strftime("%Y%m%d_%H%M%S")


def generate_detailed_timestamp():
    """ç”Ÿæˆè¯¦ç»†çš„æ—¶é—´æˆ³å­—ç¬¦ä¸²"""
    return datetime.now().strftime("%Y-%m-%d_%H-%M-%S")


class Exp_Fused_Forecast(Exp_Basic):
    def __init__(self, args):
        super(Exp_Fused_Forecast, self).__init__(args)
        self.experiment_timestamp = generate_timestamp()
        self.detailed_timestamp = generate_detailed_timestamp()
        print(f"å®éªŒæ—¶é—´æˆ³: {self.experiment_timestamp}")
        print(f"è¯¦ç»†æ—¶é—´æˆ³: {self.detailed_timestamp}")

    def _build_model(self):
        model = self.model_dict[self.args.model].Model(self.args).float()
        if self.args.use_multi_gpu and self.args.use_gpu:
            model = nn.DataParallel(model, device_ids=self.args.device_ids)
        return model

    def _get_data(self, flag):
        data_set, data_loader = data_provider(self.args, flag)
        return data_set, data_loader

    def _select_criterion(self):
        """é€‰æ‹©æŸå¤±å‡½æ•°"""
        if getattr(self.args, 'use_adaptive_loss', False):
            print("ğŸ¯ ä½¿ç”¨Barronè‡ªé€‚åº”æŸå¤±å‡½æ•°")
            adaptive_loss = BarronAdaptiveLoss(alpha_init=2.0, scale_init=1.0)
            if hasattr(self, 'device'):
                adaptive_loss = adaptive_loss.to(self.device)
            elif torch.cuda.is_available():
                adaptive_loss = adaptive_loss.cuda()
            return adaptive_loss
        else:
            print("ğŸ“Š ä½¿ç”¨æ ‡å‡†MSEæŸå¤±å‡½æ•°")
            return nn.MSELoss()

    def _select_optimizer(self):
        """é€‰æ‹©ä¼˜åŒ–å™¨ï¼ŒåŒ…å«æŸå¤±å‡½æ•°å‚æ•°"""
        criterion = self._select_criterion()

        # æ”¶é›†æ‰€æœ‰éœ€è¦ä¼˜åŒ–çš„å‚æ•°
        model_params = list(self.model.parameters())

        if isinstance(criterion, BarronAdaptiveLoss):
            # åŒ…å«æŸå¤±å‡½æ•°çš„å‚æ•°
            loss_params = list(criterion.parameters())
            all_params = model_params + loss_params
            print(f"âœ… ä¼˜åŒ–å™¨åŒ…å«: æ¨¡å‹å‚æ•° {len(model_params)} + æŸå¤±å‚æ•° {len(loss_params)} = {len(all_params)}")
        else:
            all_params = model_params
            print(f"ğŸ“Š ä¼˜åŒ–å™¨åŒ…å«: æ¨¡å‹å‚æ•° {len(model_params)}")

        model_optim = optim.Adam(all_params, lr=self.args.learning_rate)

        # ä¿å­˜criterionä»¥ä¾›åç»­ä½¿ç”¨
        self.criterion = criterion

        return model_optim

    def vali(self, vali_data, vali_loader, criterion):
        total_loss = []
        preds = []
        trues = []

        self.model.eval()
        with torch.no_grad():
            for i, (batch_x, batch_y, batch_x_mark, batch_y_mark) in enumerate(vali_loader):
                batch_x = batch_x.float().to(self.device)
                batch_y = batch_y.float().to(self.device)
                batch_x_mark = batch_x_mark.float().to(self.device)
                batch_y_mark = batch_y_mark.float().to(self.device)

                outputs = self.model(batch_x, batch_x_mark, None, batch_y_mark)

                f_dim = -1 if self.args.features == 'MS' else 0
                outputs = outputs[:, -self.args.pred_len:, f_dim:]
                batch_y = batch_y[:, -self.args.pred_len:, f_dim:]

                loss = criterion(outputs, batch_y)
                total_loss.append(loss.item())

                preds.append(outputs.detach().cpu().numpy())
                trues.append(batch_y.detach().cpu().numpy())

        total_loss = np.average(total_loss)

        if len(preds) == 0:
            print("Warning: No predictions generated during validation")
            print(f"Validation data length: {len(vali_data)}")
            print(f"Validation loader length: {len(vali_loader)}")
            return float('inf'), 0.0

        preds = np.concatenate(preds, axis=0)
        trues = np.concatenate(trues, axis=0)
        _, _, _, _, _ = metric(preds, trues)
        from sklearn.metrics import r2_score
        r2 = r2_score(trues.flatten(), preds.flatten())

        self.model.train()
        return total_loss, r2

    def train(self, setting):
        """ä¿®æ”¹åçš„è®­ç»ƒæ–¹æ³•ï¼Œé›†æˆè‡ªé€‚åº”æŸå¤±å’Œå­¦ä¹ ç‡è°ƒåº¦å™¨"""
        train_data, train_loader = self._get_data(flag='train')
        vali_data, vali_loader = self._get_data(flag='val')
        test_data, test_loader = self._get_data(flag='test')

        timestamped_setting = f"{setting}_{self.experiment_timestamp}"
        path = os.path.join(self.args.checkpoints, timestamped_setting)
        if not os.path.exists(path):
            os.makedirs(path)

        time_now = time.time()
        train_steps = len(train_loader)

        early_stopping = EarlyStopping(patience=3, verbose=True)
        max_epochs = 10

        # é‡è¦ï¼šå…ˆé€‰æ‹©ä¼˜åŒ–å™¨ï¼ˆå†…éƒ¨ä¼šè°ƒç”¨_select_criterionï¼‰
        model_optim = self._select_optimizer()
        criterion = self.criterion  # ä½¿ç”¨ä¿å­˜çš„criterion

        # è¾“å‡ºæŸå¤±å‡½æ•°ä¿¡æ¯
        if isinstance(criterion, BarronAdaptiveLoss):
            params = criterion.get_params()
            print(f"ğŸ“‹ è‡ªé€‚åº”æŸå¤±åˆå§‹å‚æ•°: Î±={params['alpha']:.3f}, Ïƒ={params['scale']:.3f}")

        # ===== å­¦ä¹ ç‡è°ƒåº¦å™¨è®¾ç½® =====
        if hasattr(self.args, 'lradj') and self.args.lradj == 'adaptive':
            scheduler = AdaptiveLossLRScheduler(
                optimizer=model_optim,
                patience=1,
                factor=0.5,
                min_lr=1e-7,
                verbose=True,
                threshold=1e-4,
                cooldown=2
            )
            use_adaptive = True

        elif hasattr(self.args, 'lradj') and self.args.lradj == 'combined':
            scheduler = CombinedLRScheduler(
                optimizer=model_optim,
                T_max=max_epochs,
                eta_min=1e-6,
                adaptive_patience=1,
                adaptive_factor=0.3,
                min_lr=1e-8,
                verbose=True
            )
            use_adaptive = True

        elif hasattr(self.args, 'lradj') and self.args.lradj == 'plateau':
            from torch.optim.lr_scheduler import ReduceLROnPlateau
            scheduler = ReduceLROnPlateau(
                optimizer=model_optim,
                mode='min',
                factor=0.5,
                patience=1,
                verbose=True,
                threshold=1e-4,
                min_lr=1e-7
            )
            use_adaptive = True

        else:
            if hasattr(self.args, 'lradj') and self.args.lradj == 'TST':
                scheduler = lr_scheduler.OneCycleLR(
                    optimizer=model_optim,
                    steps_per_epoch=train_steps,
                    pct_start=self.args.pct_start,
                    epochs=max_epochs,
                    max_lr=self.args.learning_rate
                )
            else:
                scheduler = None
            use_adaptive = False

        # è®°å½•è®­ç»ƒå†å²
        train_history = {
            'train_loss': [],
            'vali_loss': [],
            'vali_r2': [],
            'test_loss': [],
            'test_r2': [],
            'learning_rate': []
        }

        # å¦‚æœä½¿ç”¨è‡ªé€‚åº”æŸå¤±ï¼Œè®°å½•æŸå¤±å‚æ•°å˜åŒ–
        if isinstance(criterion, BarronAdaptiveLoss):
            train_history['loss_alpha'] = []
            train_history['loss_scale'] = []

        # åˆ›å»ºæ€»çš„epochè¿›åº¦æ¡
        epoch_pbar = tqdm(range(max_epochs), desc="Training Epochs", unit="epoch")

        for epoch in epoch_pbar:
            iter_count = 0
            train_loss = []
            self.model.train()
            epoch_time = time.time()

            # ä¸ºæ¯ä¸ªepochçš„batchåˆ›å»ºè¿›åº¦æ¡
            batch_pbar = tqdm(train_loader, desc=f"Epoch {epoch + 1}/{max_epochs}",
                              leave=False, unit="batch")

            for i, (batch_x, batch_y, batch_x_mark, batch_y_mark) in enumerate(batch_pbar):
                iter_count += 1
                model_optim.zero_grad()

                batch_x = batch_x.float().to(self.device)
                batch_y = batch_y.float().to(self.device)
                batch_x_mark = batch_x_mark.float().to(self.device)
                batch_y_mark = batch_y_mark.float().to(self.device)

                outputs = self.model(batch_x, batch_x_mark, None, batch_y_mark)

                f_dim = -1 if self.args.features == 'MS' else 0
                outputs = outputs[:, -self.args.pred_len:, f_dim:]
                batch_y = batch_y[:, -self.args.pred_len:, f_dim:]

                loss = criterion(outputs, batch_y)
                train_loss.append(loss.item())

                # æ›´æ–°è¿›åº¦æ¡æ˜¾ç¤º
                current_lr = model_optim.param_groups[0]['lr']

                if isinstance(criterion, BarronAdaptiveLoss):
                    params = criterion.get_params()
                    batch_pbar.set_postfix({
                        'Loss': f"{loss.item():.6f}",
                        'LR': f"{current_lr:.2e}",
                        'Î±': f"{params['alpha']:.3f}",
                        'Ïƒ': f"{params['scale']:.3f}"
                    })
                else:
                    batch_pbar.set_postfix({
                        'Loss': f"{loss.item():.6f}",
                        'LR': f"{current_lr:.2e}"
                    })

                if (i + 1) % 100 == 0:
                    speed = (time.time() - time_now) / iter_count
                    left_time = speed * ((max_epochs - epoch) * train_steps - i)
                    tqdm.write(f"\titers: {i + 1}, epoch: {epoch + 1} | loss: {loss.item():.7f} | lr: {current_lr:.2e}")
                    tqdm.write(f'\tspeed: {speed:.4f}s/iter; left time: {left_time:.4f}s')

                    # è¾“å‡ºè‡ªé€‚åº”æŸå¤±å‚æ•°
                    if isinstance(criterion, BarronAdaptiveLoss):
                        params = criterion.get_params()
                        tqdm.write(f'\tè‡ªé€‚åº”æŸå¤±: Î±={params["alpha"]:.4f}, Ïƒ={params["scale"]:.4f}')

                    iter_count = 0
                    time_now = time.time()

                loss.backward()
                model_optim.step()

                # å¯¹äºOneCycleLRï¼Œåœ¨æ¯ä¸ªbatchåè°ƒç”¨
                if hasattr(self.args, 'lradj') and self.args.lradj == 'TST' and scheduler is not None:
                    scheduler.step()

            batch_pbar.close()

            epoch_cost_time = time.time() - epoch_time
            train_loss = np.average(train_loss)
            vali_loss, vali_r2 = self.vali(vali_data, vali_loader, criterion)
            test_loss, test_r2 = self.vali(test_data, test_loader, criterion)

            # è®°å½•å½“å‰å­¦ä¹ ç‡
            current_lr = model_optim.param_groups[0]['lr']

            # è®°å½•å†å²
            train_history['train_loss'].append(train_loss)
            train_history['vali_loss'].append(vali_loss)
            train_history['vali_r2'].append(vali_r2)
            train_history['test_loss'].append(test_loss)
            train_history['test_r2'].append(test_r2)
            train_history['learning_rate'].append(current_lr)

            # å¦‚æœä½¿ç”¨è‡ªé€‚åº”æŸå¤±ï¼Œè®°å½•æŸå¤±å‚æ•°
            if isinstance(criterion, BarronAdaptiveLoss):
                params = criterion.get_params()
                train_history['loss_alpha'].append(params['alpha'])
                train_history['loss_scale'].append(params['scale'])

            # ===== å­¦ä¹ ç‡è°ƒåº¦ =====
            if use_adaptive:
                if hasattr(self.args, 'lradj') and self.args.lradj in ['adaptive', 'combined']:
                    scheduler.step(vali_loss, epoch)
                elif hasattr(self.args, 'lradj') and self.args.lradj == 'plateau':
                    scheduler.step(vali_loss)
            else:
                if hasattr(self.args, 'lradj') and self.args.lradj != 'TST':
                    adjust_learning_rate(model_optim, epoch + 1, self.args)

            # æ£€æŸ¥å­¦ä¹ ç‡å˜åŒ–
            new_lr = model_optim.param_groups[0]['lr']
            if abs(new_lr - current_lr) > 1e-10:
                tqdm.write(f"Learning rate changed from {current_lr:.2e} to {new_lr:.2e}")

            # è¾“å‡ºepochç»“æœ
            tqdm.write(f"Epoch: {epoch + 1} cost time: {epoch_cost_time:.2f}s")
            tqdm.write(
                f"Epoch: {epoch + 1}, Steps: {train_steps} | Train Loss: {train_loss:.7f} Vali Loss: {vali_loss:.7f} Test Loss: {test_loss:.7f}")
            tqdm.write(f"Vali RÂ²: {vali_r2:.4f} Test RÂ²: {test_r2:.4f} | LR: {new_lr:.2e}")

            # è¾“å‡ºè‡ªé€‚åº”æŸå¤±å‚æ•°å˜åŒ–
            if isinstance(criterion, BarronAdaptiveLoss):
                params = criterion.get_params()
                tqdm.write(f"è‡ªé€‚åº”æŸå¤±å‚æ•°: Î±={params['alpha']:.4f}, Ïƒ={params['scale']:.4f}")

            # æ›´æ–°epochè¿›åº¦æ¡
            if isinstance(criterion, BarronAdaptiveLoss):
                params = criterion.get_params()
                epoch_pbar.set_postfix({
                    'Train_Loss': f"{train_loss:.6f}",
                    'Vali_R2': f"{vali_r2:.4f}",
                    'Test_R2': f"{test_r2:.4f}",
                    'Î±': f"{params['alpha']:.3f}",
                    'Ïƒ': f"{params['scale']:.3f}"
                })
            else:
                epoch_pbar.set_postfix({
                    'Train_Loss': f"{train_loss:.6f}",
                    'Vali_R2': f"{vali_r2:.4f}",
                    'Test_R2': f"{test_r2:.4f}",
                    'LR': f"{new_lr:.2e}"
                })

            # æ—©åœæ£€æŸ¥
            early_stopping(vali_loss, self.model, path)
            if early_stopping.early_stop:
                tqdm.write("Early stopping triggered!")
                tqdm.write(f"Training stopped at epoch {epoch + 1}")
                break

        epoch_pbar.close()

        # ä¿å­˜è®­ç»ƒå†å²
        history_file = os.path.join(path, f'training_history_{self.experiment_timestamp}.json')
        train_history_serializable = {}
        for key, value in train_history.items():
            if isinstance(value[0], (int, float)):
                train_history_serializable[key] = [float(x) for x in value]
            else:
                train_history_serializable[key] = value

        import json
        with open(history_file, 'w') as f:
            json.dump(train_history_serializable, f, indent=2)

        # å¦‚æœä½¿ç”¨è‡ªé€‚åº”è°ƒåº¦å™¨ï¼Œä¿å­˜å¯è§†åŒ–
        if use_adaptive and isinstance(scheduler, (AdaptiveLossLRScheduler, CombinedLRScheduler)):
            try:
                scheduler_state_file = os.path.join(path, f'scheduler_state_{self.experiment_timestamp}.json')
                if hasattr(scheduler, 'state_dict'):
                    with open(scheduler_state_file, 'w') as f:
                        json.dump(scheduler.state_dict(), f, indent=2)

                lr_plot_path = os.path.join(path, f'lr_loss_history_{self.experiment_timestamp}.png')
                plot_lr_loss_history(scheduler, lr_plot_path)
            except Exception as e:
                tqdm.write(f"Warning: Could not save scheduler visualization: {e}")

        best_model_path = path + '/' + 'checkpoint.pth'
        self.model.load_state_dict(torch.load(best_model_path))

        # æœ€ç»ˆè®­ç»ƒå®Œæˆåè®¡ç®—è®­ç»ƒé›†RÂ²
        tqdm.write("Computing final training metrics...")
        final_train_loss, final_train_r2 = self.train_step_metrics(train_loader, criterion)
        tqdm.write(f"Final Training RÂ²: {final_train_r2:.6f}")

        # è¾“å‡ºå­¦ä¹ ç‡è°ƒåº¦æ€»ç»“
        if use_adaptive:
            final_lr = model_optim.param_groups[0]['lr']
            initial_lr = self.args.learning_rate
            tqdm.write(f"Learning rate summary: Initial: {initial_lr:.2e}, Final: {final_lr:.2e}")
            if hasattr(scheduler, 'lr_history') and scheduler.lr_history:
                min_lr = min(scheduler.lr_history)
                tqdm.write(f"Minimum learning rate reached: {min_lr:.2e}")

        # è¾“å‡ºè‡ªé€‚åº”æŸå¤±æ€»ç»“
        if isinstance(criterion, BarronAdaptiveLoss):
            final_params = criterion.get_params()
            tqdm.write(f"è‡ªé€‚åº”æŸå¤±æœ€ç»ˆå‚æ•°: Î±={final_params['alpha']:.4f}, Ïƒ={final_params['scale']:.4f}")

    def train_step_metrics(self, train_loader, criterion):
        """è®¡ç®—è®­ç»ƒé›†ä¸Šçš„RÂ²æŒ‡æ ‡ï¼ˆå¯é€‰è°ƒç”¨ï¼‰"""
        train_preds = []
        train_trues = []
        train_loss = []

        self.model.eval()
        with torch.no_grad():
            metric_pbar = tqdm(enumerate(train_loader), desc="Computing train metrics",
                               total=min(100, len(train_loader)), leave=False)

            for i, (batch_x, batch_y, batch_x_mark, batch_y_mark) in metric_pbar:
                if i >= 100:
                    break

                batch_x = batch_x.float().to(self.device)
                batch_y = batch_y.float().to(self.device)
                batch_x_mark = batch_x_mark.float().to(self.device)
                batch_y_mark = batch_y_mark.float().to(self.device)

                outputs = self.model(batch_x, batch_x_mark, None, batch_y_mark)

                f_dim = -1 if self.args.features == 'MS' else 0
                outputs = outputs[:, -self.args.pred_len:, f_dim:]
                batch_y = batch_y[:, -self.args.pred_len:, f_dim:]

                loss = criterion(outputs, batch_y)
                train_loss.append(loss.item())

                train_preds.append(outputs.detach().cpu().numpy())
                train_trues.append(batch_y.detach().cpu().numpy())

            metric_pbar.close()

        if train_preds:
            train_preds = np.concatenate(train_preds, axis=0)
            train_trues = np.concatenate(train_trues, axis=0)
            _, _, _, _, _ = metric(train_preds, train_trues)
            from sklearn.metrics import r2_score
            train_r2 = r2_score(train_trues.flatten(), train_preds.flatten())
        else:
            train_r2 = 0.0

        self.model.train()
        return np.average(train_loss), train_r2

    def test(self, setting, test=0):
        test_data, test_loader = self._get_data(flag='test')

        timestamped_setting = f"{setting}_{self.experiment_timestamp}"

        if test:
            print('loading model')
            self.model.load_state_dict(
                torch.load(os.path.join('./checkpoints/' + timestamped_setting, 'checkpoint.pth')))

        preds = []
        trues = []

        folder_path = './test_results/' + timestamped_setting + '/'
        if not os.path.exists(folder_path):
            os.makedirs(folder_path)

        self.model.eval()
        with torch.no_grad():
            test_pbar = tqdm(test_loader, desc="Testing", unit="batch")

            for i, (batch_x, batch_y, batch_x_mark, batch_y_mark) in enumerate(test_pbar):
                batch_x = batch_x.float().to(self.device)
                batch_y = batch_y.float().to(self.device)
                batch_x_mark = batch_x_mark.float().to(self.device)
                batch_y_mark = batch_y_mark.float().to(self.device)

                outputs = self.model(batch_x, batch_x_mark, None, batch_y_mark)

                f_dim = -1 if self.args.features == 'MS' else 0
                outputs = outputs[:, -self.args.pred_len:, f_dim:]
                batch_y = batch_y[:, -self.args.pred_len:, f_dim:]

                pred = outputs.detach().cpu().numpy()
                true = batch_y.detach().cpu().numpy()

                preds.append(pred)
                trues.append(true)

            test_pbar.close()

        preds = np.concatenate(preds, axis=0)
        trues = np.concatenate(trues, axis=0)

        mae, mse, rmse, mape, mspe = metric(preds, trues)
        from sklearn.metrics import r2_score
        r2 = r2_score(trues.flatten(), preds.flatten())
        print(f'mse:{mse:.6f}, mae:{mae:.6f}')
        print(f'rmse:{rmse:.6f}, mape:{mape:.6f}, mspe:{mspe:.6f}')
        print(f'RÂ²:{r2:.6f}')

    def test(self, setting, test=0):
        test_data, test_loader = self._get_data(flag='test')

        # ä¿®æ”¹ï¼šæ·»åŠ æ—¶é—´æˆ³åˆ°è®¾ç½®
        timestamped_setting = f"{setting}_{self.experiment_timestamp}"

        if test:
            print('loading model')
            self.model.load_state_dict(
                torch.load(os.path.join('./checkpoints/' + timestamped_setting, 'checkpoint.pth')))

        preds = []
        trues = []

        # ä¿®æ”¹ï¼šåˆ›å»ºå¸¦æ—¶é—´æˆ³çš„æµ‹è¯•ç»“æœæ–‡ä»¶å¤¹
        folder_path = './test_results/' + timestamped_setting + '/'
        if not os.path.exists(folder_path):
            os.makedirs(folder_path)

        self.model.eval()
        with torch.no_grad():
            test_pbar = tqdm(test_loader, desc="Testing", unit="batch")

            for i, (batch_x, batch_y, batch_x_mark, batch_y_mark) in enumerate(test_pbar):
                batch_x = batch_x.float().to(self.device)
                batch_y = batch_y.float().to(self.device)
                batch_x_mark = batch_x_mark.float().to(self.device)
                batch_y_mark = batch_y_mark.float().to(self.device)

                outputs = self.model(batch_x, batch_x_mark, None, batch_y_mark)

                f_dim = -1 if self.args.features == 'MS' else 0
                outputs = outputs[:, -self.args.pred_len:, f_dim:]
                batch_y = batch_y[:, -self.args.pred_len:, f_dim:]

                pred = outputs.detach().cpu().numpy()
                true = batch_y.detach().cpu().numpy()

                preds.append(pred)
                trues.append(true)

            test_pbar.close()

        preds = np.concatenate(preds, axis=0)
        trues = np.concatenate(trues, axis=0)

        mae, mse, rmse, mape, mspe = metric(preds, trues)
        from sklearn.metrics import r2_score
        r2 = r2_score(trues.flatten(), preds.flatten())
        print(f'mse:{mse:.6f}, mae:{mae:.6f}')
        print(f'rmse:{rmse:.6f}, mape:{mape:.6f}, mspe:{mspe:.6f}')
        print(f'RÂ²:{r2:.6f}')

        # ä¿®æ”¹ï¼šä¿å­˜ç»“æœåˆ°å¸¦æ—¶é—´æˆ³çš„æ–‡ä»¶å¤¹
        folder_path = './results/' + timestamped_setting + '/'
        if not os.path.exists(folder_path):
            os.makedirs(folder_path)

        # === åå½’ä¸€åŒ–é€»è¾‘ä¿æŒä¸å˜ ===
        print("å¼€å§‹åå½’ä¸€åŒ–å¤„ç†...")
        print(f"åŸå§‹predså½¢çŠ¶: {preds.shape}")
        print(f"åŸå§‹trueså½¢çŠ¶: {trues.shape}")

        # è·å–åŸå§‹æµ‹è¯•æ•°æ®
        raw_df = test_data.raw_test_df
        print("åŸå§‹æ•°æ®åˆ—å:", raw_df.columns.tolist())

        # è®¡ç®—é¢„æµ‹æ•°æ®çš„æ•°é‡
        num_preds = len(preds.flatten())
        print(f"é¢„æµ‹æ•°æ®ç‚¹æ•°é‡: {num_preds}")

        # æ–¹æ³•1ï¼šå¦‚æœfeatures=='S'ï¼ˆå•å˜é‡ï¼‰ï¼Œç›´æ¥åå½’ä¸€åŒ–
        if self.args.features == 'S':
            print("ä½¿ç”¨å•å˜é‡åå½’ä¸€åŒ–æ–¹æ³•")
            preds_unscaled = test_data.inverse_transform(preds.reshape(-1, 1)).flatten()
            trues_unscaled = test_data.inverse_transform(trues.reshape(-1, 1)).flatten()
        else:
            print("ä½¿ç”¨å¤šå˜é‡åå½’ä¸€åŒ–æ–¹æ³•")
            feature_dim = test_data.data_x.shape[-1]
            print(f"ç‰¹å¾ç»´åº¦: {feature_dim}")

            if hasattr(test_data, 'scaler'):
                test_mean = np.mean(test_data.data_x, axis=0)

                preds_full = np.tile(test_mean, (num_preds, 1))
                preds_full[:, -1] = preds.flatten()

                trues_full = np.tile(test_mean, (num_preds, 1))
                trues_full[:, -1] = trues.flatten()

                preds_unscaled = test_data.inverse_transform(preds_full)[:, -1]
                trues_unscaled = test_data.inverse_transform(trues_full)[:, -1]
            else:
                print("è­¦å‘Šï¼šæ— æ³•æ‰¾åˆ°scalerï¼Œä½¿ç”¨åŸå§‹æ•°æ®")
                preds_unscaled = preds.flatten()
                trues_unscaled = trues.flatten()

        print(f"åå½’ä¸€åŒ–åé¢„æµ‹å€¼èŒƒå›´: [{preds_unscaled.min():.6f}, {preds_unscaled.max():.6f}]")
        print(f"åå½’ä¸€åŒ–åçœŸå®å€¼èŒƒå›´: [{trues_unscaled.min():.6f}, {trues_unscaled.max():.6f}]")

        # è·å–å¯¹åº”çš„cycleæ•°æ®
        start_idx = self.args.seq_len
        end_idx = start_idx + num_preds

        print(f"ä»åŸå§‹æ•°æ®è·å–cycleï¼Œç´¢å¼•èŒƒå›´: {start_idx} åˆ° {end_idx}")
        print(f"åŸå§‹æ•°æ®é•¿åº¦: {len(raw_df)}")

        # æ£€æŸ¥æ•°æ®é›†ä¸­æ˜¯å¦æœ‰Cycleåˆ—
        cycle_col = None
        date_col = None

        possible_cycle_names = ['Cycle', 'cycle', 'CYCLE', 'cycle_number', 'Cycle_Number']
        for col in possible_cycle_names:
            if col in raw_df.columns:
                cycle_col = col
                break

        possible_date_names = ['date', 'Date', 'DATE', 'time', 'Time', 'timestamp']
        for col in possible_date_names:
            if col in raw_df.columns:
                date_col = col
                break

        print(f"æ‰¾åˆ°çš„Cycleåˆ—: {cycle_col}")
        print(f"æ‰¾åˆ°çš„Dateåˆ—: {date_col}")

        if end_idx <= len(raw_df):
            if cycle_col is not None:
                cycle_data = raw_df[cycle_col].values[start_idx:end_idx]
                print("ä½¿ç”¨æ•°æ®é›†ä¸­çš„Cycleåˆ—")
            else:
                cycle_data = np.arange(start_idx + 1, end_idx + 1)
                print("æ•°æ®é›†ä¸­æ²¡æœ‰Cycleåˆ—ï¼Œä½¿ç”¨ç”Ÿæˆçš„åºå·")

            if date_col is not None:
                date_data = raw_df[date_col].values[start_idx:end_idx]
            else:
                date_data = None

            true_targets = raw_df[self.args.target].values[start_idx:end_idx]
        else:
            print("è­¦å‘Šï¼šç´¢å¼•è¶…å‡ºèŒƒå›´ï¼Œè°ƒæ•´ç´¢å¼•")
            if cycle_col is not None:
                cycle_data = raw_df[cycle_col].values[-num_preds:]
            else:
                cycle_data = np.arange(len(raw_df) - num_preds + 1, len(raw_df) + 1)

            if date_col is not None:
                date_data = raw_df[date_col].values[-num_preds:]
            else:
                date_data = None

            true_targets = raw_df[self.args.target].values[-num_preds:]

        print(f"è·å–çš„cycleæ•°æ®é•¿åº¦: {len(cycle_data)}")
        print(f"cycleæ•°æ®ç±»å‹: {type(cycle_data[0])}")
        print(f"cycleæ•°æ®å‰5ä¸ª: {cycle_data[:5]}")
        print(f"è·å–çš„çœŸå®targeté•¿åº¦: {len(true_targets)}")
        print(f"çœŸå®targetèŒƒå›´: [{true_targets.min():.6f}, {true_targets.max():.6f}]")

        min_length = min(len(cycle_data), len(true_targets), len(preds_unscaled))
        print(f"æœ€ç»ˆä½¿ç”¨çš„æ•°æ®é•¿åº¦: {min_length}")

        # ä¿®æ”¹ï¼šåˆ›å»ºç»“æœDataFrameæ—¶æ·»åŠ å®éªŒä¿¡æ¯
        results_df = pd.DataFrame({
            'Cycle': cycle_data[:min_length],
            'True_Target': true_targets[:min_length],
            'Predicted_Target': preds_unscaled[:min_length]
        })

        # æ·»åŠ å®éªŒå…ƒä¿¡æ¯
        experiment_info = pd.DataFrame({
            'Experiment_Timestamp': [self.detailed_timestamp] * min_length,
            'Model_Name': [self.args.model] * min_length,
            'Dataset': [self.args.data] * min_length,
            'Data_Path': [self.args.data_path] * min_length,
            'MSE': [mse] * min_length,
            'MAE': [mae] * min_length,
            'RMSE': [rmse] * min_length,
            'R2': [r2] * min_length,
            'Final_d_model': [self.args.d_model] * min_length,
            'Final_learning_rate': [self.args.learning_rate] * min_length,
            'Final_dropout': [self.args.dropout] * min_length,
        })

        # åˆå¹¶å®éªŒä¿¡æ¯å’Œç»“æœ
        detailed_results_df = pd.concat([experiment_info, results_df], axis=1)

        # ä¿å­˜è¯¦ç»†ç»“æœCSVæ–‡ä»¶
        results_csv_path = os.path.join(folder_path, f'forecast_results_{self.experiment_timestamp}.csv')
        detailed_results_df.to_csv(results_csv_path, index=False)
        print(f"è¯¦ç»†ç»“æœå·²ä¿å­˜è‡³ {results_csv_path}")

        # åŒæ—¶ä¿å­˜ç®€åŒ–ç‰ˆæœ¬ï¼ˆä¿æŒåŸæœ‰æ ¼å¼å…¼å®¹æ€§ï¼‰
        simple_results_csv_path = os.path.join(folder_path, 'forecast_results.csv')
        results_df.to_csv(simple_results_csv_path, index=False)
        print(f"ç®€åŒ–ç»“æœå·²ä¿å­˜è‡³ {simple_results_csv_path}")

        print("å‰5è¡Œç»“æœé¢„è§ˆ:")
        print(results_df.head())

        print("å¼€å§‹ç”Ÿæˆå¯è§†åŒ–å›¾è¡¨...")

        # è®¾ç½®matplotlibçš„ç§‘æŠ€è®ºæ–‡é£æ ¼
        plt.style.use('seaborn-v0_8-whitegrid')
        sns.set_palette("husl")

        # åˆ›å»ºå›¾å½¢å’Œè½´
        fig, ax = plt.subplots(figsize=(12, 8), dpi=300)

        # ç¡®å®šxè½´æ•°æ®ï¼šä¼˜å…ˆä½¿ç”¨Cycleï¼Œå¦‚æœç”¨æˆ·æ˜ç¡®éœ€è¦dateåˆ™ä½¿ç”¨date
        # è¿™é‡Œæˆ‘ä»¬ä½¿ç”¨Cycleä½œä¸ºxè½´ï¼Œå› ä¸ºè¿™æ˜¯ç”µæ± ç ”ç©¶çš„æ ‡å‡†åšæ³•
        x_data = cycle_data[:min_length]
        x_label = 'Cycle'

        print(f"ä½¿ç”¨xè½´æ•°æ®: {x_label}")
        print(f"xè½´æ•°æ®èŒƒå›´: {x_data.min()} åˆ° {x_data.max()}")

        # ç»˜åˆ¶çœŸå®å€¼ï¼ˆè“çº¿ï¼‰
        ax.plot(x_data, true_targets[:min_length],
                color='#2E86AB', linewidth=2.5, alpha=0.8,
                label='True SoH', marker='o', markersize=3, markevery=max(1, min_length // 50))

        # ç»˜åˆ¶é¢„æµ‹å€¼ï¼ˆçº¢çº¿ï¼‰
        ax.plot(x_data, preds_unscaled[:min_length],
                color='#F24236', linewidth=2.5, alpha=0.8,
                label='Predicted SoH', marker='s', markersize=3, markevery=max(1, min_length // 50))

        # è®¾ç½®æ ‡é¢˜å’Œæ ‡ç­¾
        ax.set_title('Battery SoH Prediction - Dual Branch Model', fontsize=20, fontweight='bold', pad=20)
        ax.set_ylabel('State of Health (SoH)', fontsize=14, fontweight='bold')
        ax.set_xlabel(x_label, fontsize=14, fontweight='bold')

        # è®¾ç½®ç½‘æ ¼
        ax.grid(True, alpha=0.3, linestyle='--', linewidth=0.8)
        ax.set_axisbelow(True)

        # æ·»åŠ å›¾ä¾‹ï¼ˆå³ä¸Šè§’ï¼‰
        legend = ax.legend(loc='upper right', fontsize=12, frameon=True,
                           fancybox=True, shadow=True, framealpha=0.9,
                           bbox_to_anchor=(0.98, 0.98))
        legend.get_frame().set_facecolor('white')
        legend.get_frame().set_edgecolor('gray')
        legend.get_frame().set_linewidth(0.8)

        # æ·»åŠ æ€§èƒ½æŒ‡æ ‡æ–‡æœ¬æ¡†ï¼ˆå³ä¸Šè§’ï¼Œå›¾ä¾‹ä¸‹æ–¹ï¼‰
        textstr = f'MAE: {mae:.4f}\nMSE: {mse:.4f}\nRMSE: {rmse:.4f}\nRÂ²: {r2:.4f}'
        props = dict(boxstyle='round,pad=0.5', facecolor='lightgray', alpha=0.8, edgecolor='gray')
        ax.text(0.98, 0.75, textstr, transform=ax.transAxes, fontsize=10,
                verticalalignment='top', horizontalalignment='right',
                bbox=props, family='monospace')

        # è®¾ç½®è½´çš„èŒƒå›´å’Œåˆ»åº¦
        y_min, y_max = min(np.min(true_targets[:min_length]), np.min(preds_unscaled[:min_length])), \
            max(np.max(true_targets[:min_length]), np.max(preds_unscaled[:min_length]))
        y_range = y_max - y_min
        ax.set_ylim(y_min - 0.05 * y_range, y_max + 0.05 * y_range)

        # ç¾åŒ–åˆ»åº¦
        ax.tick_params(axis='both', which='major', labelsize=11, width=1.2, length=6)
        ax.tick_params(axis='both', which='minor', width=0.8, length=3)

        # è®¾ç½®è¾¹æ¡†
        for spine in ax.spines.values():
            spine.set_linewidth(1.2)
            spine.set_color('gray')

        # æ·»åŠ é˜´å½±åŒºåŸŸæ˜¾ç¤ºé¢„æµ‹è¯¯å·®
        error = np.abs(true_targets[:min_length] - preds_unscaled[:min_length])
        ax.fill_between(x_data,
                        preds_unscaled[:min_length] - error / 2,
                        preds_unscaled[:min_length] + error / 2,
                        alpha=0.2, color='red', label='Prediction Error Band')

        # è°ƒæ•´å¸ƒå±€
        plt.tight_layout()

        # ä¿å­˜é«˜è´¨é‡å›¾ç‰‡
        plot_path = os.path.join(folder_path, 'dual_branch_battery_soh_prediction.png')
        plt.savefig(plot_path, dpi=300, bbox_inches='tight',
                    facecolor='white', edgecolor='none')

        # åŒæ—¶ä¿å­˜PDFæ ¼å¼ï¼ˆé€‚åˆè®ºæ–‡ä½¿ç”¨ï¼‰
        pdf_path = os.path.join(folder_path, 'dual_branch_battery_soh_prediction.pdf')
        plt.savefig(pdf_path, bbox_inches='tight',
                    facecolor='white', edgecolor='none')

        print(f"å¯è§†åŒ–å›¾è¡¨å·²ä¿å­˜:")
        print(f"PNGæ ¼å¼: {plot_path}")
        print(f"PDFæ ¼å¼: {pdf_path}")

        # æ˜¾ç¤ºå›¾è¡¨ï¼ˆå¯é€‰ï¼Œå¦‚æœåœ¨jupyter notebookä¸­è¿è¡Œï¼‰
        plt.show()
        plt.close()

        # =================== å¯è§†åŒ–åŠŸèƒ½ç»“æŸ ===================

        # ä¿å­˜æŒ‡æ ‡
        np.save(folder_path + f'metrics_{self.experiment_timestamp}.npy', np.array([mae, mse, rmse, mape, mspe, r2]))
        np.save(folder_path + f'pred_{self.experiment_timestamp}.npy', preds)
        np.save(folder_path + f'true_{self.experiment_timestamp}.npy', trues)

        # ä¿®æ”¹ï¼šä¿å­˜åˆ°å¸¦æ—¶é—´æˆ³çš„æ–‡æœ¬æ–‡ä»¶
        result_file = f"result_dual_branch_forecast_{self.experiment_timestamp}.txt"
        f = open(result_file, 'a', encoding='utf-8')
        f.write(f"Experiment Time: {self.detailed_timestamp}\n")
        f.write(f"Dataset: {self.args.data_path}\n")
        f.write(f"Model: {self.args.model}\n")
        f.write(timestamped_setting + "\n")
        f.write(f'mse:{mse:.6f}, mae:{mae:.6f}, rmse:{rmse:.6f}, mape:{mape:.6f}, mspe:{mspe:.6f}, R2:{r2:.6f}\n')
        f.write('\n')
        f.close()

        # åŒæ—¶ä¿å­˜åˆ°æ€»çš„ç»“æœæ–‡ä»¶ï¼ˆä¿æŒåŸæœ‰é€»è¾‘ï¼‰
        f = open("result_dual_branch_forecast.txt", 'a', encoding='utf-8')
        f.write(f"[{self.detailed_timestamp}] " + timestamped_setting + "\n")
        f.write(f'mse:{mse:.6f}, mae:{mae:.6f}, rmse:{rmse:.6f}, mape:{mape:.6f}, mspe:{mspe:.6f}, R2:{r2:.6f}\n')
        f.write('\n')
        f.close()

        print("\n" + "=" * 50)
        return

