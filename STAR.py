import torch
import torch.nn as nn
import torch.nn.functional as F
from layers.Autoformer_EncDec import series_decomp
from layers.Embed import DataEmbedding_wo_pos
from layers.StandardNorm import Normalize
from layers.ChebyKANLayer import ChebyKANLinear
from layers.TimeDART_EncDec import Diffusion


class GlobalSTAR(nn.Module):
    """å…¨å±€STARæ¨¡å— - åœ¨åŸå§‹åºåˆ—ä¸Šè¿›è¡Œå…¨å±€ä¿¡æ¯èšåˆ"""

    def __init__(self, seq_len, channels, d_core=None):
        super().__init__()
        self.seq_len = seq_len
        self.channels = channels
        self.d_core = d_core if d_core is not None else channels // 2

        # æ ¸å¿ƒè¡¨ç¤ºç”Ÿæˆç½‘ç»œ - ä½œç”¨äºé€šé“ç»´åº¦
        self.core_gen = nn.Sequential(
            nn.Linear(channels, channels),
            nn.GELU(),
            nn.Linear(channels, self.d_core)
        )

        # èåˆç½‘ç»œ
        self.fusion_net = nn.Sequential(
            nn.Linear(channels + self.d_core, channels),
            nn.GELU(),
            nn.Dropout(0.1)
        )

    def stochastic_pooling(self, x):
        """SOFTSé£æ ¼çš„éšæœºæ± åŒ– - åœ¨é€šé“ç»´åº¦è¿›è¡Œèšåˆ"""
        # x: [B, T, d_core]
        if self.training:
            # è®­ç»ƒæ—¶ï¼šæŒ‰æ¦‚ç‡éšæœºé‡‡æ ·
            probs = F.softmax(x, dim=2)  # [B, T, d_core]

            # å¯¹æ¯ä¸ªæ—¶é—´æ­¥ç‹¬ç«‹é‡‡æ ·
            sampled_values = []
            for t in range(x.shape[1]):
                # å¯¹å½“å‰æ—¶é—´æ­¥çš„æ‰€æœ‰batchè¿›è¡Œé‡‡æ ·
                t_probs = probs[:, t, :]  # [B, d_core]
                t_values = x[:, t, :]  # [B, d_core]

                # ä¸ºæ¯ä¸ªbatchæ ·æœ¬é‡‡æ ·ä¸€ä¸ªæ ¸å¿ƒå€¼
                sampled_indices = torch.multinomial(t_probs, 1).squeeze(-1)  # [B]
                batch_indices = torch.arange(x.shape[0], device=x.device)
                sampled_t = t_values[batch_indices, sampled_indices].unsqueeze(1)  # [B, 1]
                sampled_values.append(sampled_t)

            core = torch.cat(sampled_values, dim=1).unsqueeze(-1)  # [B, T, 1]
        else:
            # æµ‹è¯•æ—¶ï¼šåŠ æƒå¹³å‡
            weights = F.softmax(x, dim=2)  # [B, T, d_core]
            core = torch.sum(x * weights, dim=2, keepdim=True)  # [B, T, 1]

        return core

    def forward(self, x):
        """
        x: [B, T, C] - åŸå§‹æ—¶åºæ•°æ®
        è¾“å‡º: [B, T, C] - å…¨å±€ä¿¡æ¯å¢å¼ºåçš„åºåˆ—
        """
        B, T, C = x.shape

        # ç”Ÿæˆæ ¸å¿ƒè¡¨ç¤ºå€™é€‰
        core_candidates = self.core_gen(x)  # [B, T, d_core]

        # éšæœºæ± åŒ–ç”Ÿæˆå…¨å±€æ ¸å¿ƒ - æ¯ä¸ªæ—¶é—´æ­¥ä¸€ä¸ªæ ¸å¿ƒå€¼
        global_core = self.stochastic_pooling(core_candidates)  # [B, T, 1]

        # å°†å…¨å±€æ ¸å¿ƒæ‰©å±•åˆ°æ‰€æœ‰é€šé“
        global_core_expanded = global_core.expand(B, T, self.d_core)  # [B, T, d_core]

        # èåˆåŸå§‹ç‰¹å¾å’Œå…¨å±€æ ¸å¿ƒ
        fused_input = torch.cat([x, global_core_expanded], dim=-1)  # [B, T, C + d_core]
        fused_output = self.fusion_net(fused_input)  # [B, T, C]

        # æ®‹å·®è¿æ¥
        return x + fused_output


class LightweightDiffusion(nn.Module):
    """è½»é‡çº§æ‰©æ•£æ¨¡å—"""

    def __init__(self, time_steps=20, device='cuda', scheduler='linear'):
        super().__init__()
        self.diffusion = Diffusion(time_steps=time_steps, device=device, scheduler=scheduler)

    def forward(self, x, apply_noise=True):
        if apply_noise and self.training:
            return self.diffusion(x)
        else:
            return x, None, None

class ResidualCNNProcessor(nn.Module):
    def __init__(self, configs):
        super().__init__()
        self.conv = nn.Conv1d(configs.d_model, configs.d_model, kernel_size=3, padding=1, groups=1)
        self.norm = nn.LayerNorm(configs.d_model)
        self.ffn = nn.Sequential(
            nn.Linear(configs.d_model, configs.d_ff),
            nn.GELU(),
            nn.Linear(configs.d_ff, configs.d_model),
            nn.Dropout(configs.dropout)
        )

    def forward(self, x):
        # x: [B, T, C]
        x_conv = self.conv(x.transpose(1, 2)).transpose(1, 2)  # [B, T, C]
        x = x + x_conv  # æ®‹å·®è¿æ¥
        x = self.norm(x)
        return self.ffn(x)




class AdaptiveKANMixer(nn.Module):
    """è‡ªé€‚åº”KANæ··åˆå™¨"""

    def __init__(self, d_model, component_type='trend'):
        super().__init__()
        # æ ¹æ®åˆ†é‡ç±»å‹é€‰æ‹©KANé˜¶æ•°
        order_map = {'trend': 6, 'seasonal': 4, 'residual': 8}
        order = order_map.get(component_type, 4)

        self.kan_layer = ChebyKANLinear(d_model, d_model, order)
        self.conv = nn.Conv1d(d_model, d_model, kernel_size=3, padding=1, groups=d_model)
        self.norm = nn.LayerNorm(d_model)

    def forward(self, x):
        B, T, C = x.shape
        x_kan = self.kan_layer(x.reshape(B * T, C)).reshape(B, T, C)
        x_conv = self.conv(x.transpose(1, 2)).transpose(1, 2)
        return self.norm(x + x_kan + x_conv)


class ComponentProcessor(nn.Module):
    """åˆ†é‡å¤„ç†å™¨ - ä¿æŒåŸæœ‰çš„KANè®¾è®¡"""

    def __init__(self, configs, component_type):
        super().__init__()
        self.component_type = component_type

        if component_type == 'trend':
            self.processor = nn.Sequential(
                AdaptiveKANMixer(configs.d_model, 'trend'),
                nn.Linear(configs.d_model, configs.d_model),
                nn.GELU(),
                nn.Dropout(configs.dropout)
            )
        elif component_type == 'seasonal':
            self.diffusion = LightweightDiffusion(time_steps=20, device=configs.device)
            self.processor = AdaptiveKANMixer(configs.d_model, 'seasonal')
        else:  # residual
            self.processor = ResidualCNNProcessor(configs)

    def forward(self, x):
        if self.component_type == 'seasonal' and self.training:
            x_noise, noise, t = self.diffusion(x, apply_noise=True)
            return self.processor(x_noise)
        else:
            return self.processor(x)


class Model(nn.Module):
    """å…¨å±€STAR + åˆ†è§£KANèåˆæ¨¡å‹"""

    def __init__(self, configs):
        super().__init__()
        self.configs = configs
        self.task_name = configs.task_name
        self.seq_len = configs.seq_len
        self.pred_len = configs.pred_len

        # å…¨å±€STARæ¨¡å— - åœ¨åˆ†è§£ä¹‹å‰è¿›è¡Œå…¨å±€ä¿¡æ¯èšåˆ
        self.global_star = GlobalSTAR(
            seq_len=configs.seq_len,
            channels=configs.enc_in,
            d_core=configs.enc_in // 2
        )

        # åˆ†è§£æ¨¡å—
        self.decomposition = series_decomp(configs.moving_avg)

        # åµŒå…¥å±‚
        if configs.channel_independence == 1:
            self.enc_embedding = DataEmbedding_wo_pos(1, configs.d_model, configs.embed, configs.freq, configs.dropout)
        else:
            self.enc_embedding = DataEmbedding_wo_pos(configs.enc_in, configs.d_model, configs.embed, configs.freq,
                                                      configs.dropout)

        # åˆ†é‡å¤„ç†å™¨ï¼ˆä¿æŒä½ çš„åˆ›æ–°è®¾è®¡ï¼‰
        self.trend_processor = ComponentProcessor(configs, 'trend')
        self.seasonal_processor = ComponentProcessor(configs, 'seasonal')
        self.residual_processor = ComponentProcessor(configs, 'residual')

        # å½’ä¸€åŒ–
        self.normalize_layers = torch.nn.ModuleList([
            Normalize(configs.enc_in, affine=True, non_norm=True if configs.use_norm == 0 else False)
            for i in range(configs.down_sampling_layers + 1)
        ])

        # é¢„æµ‹å±‚
        self.trend_predictor = nn.Linear(configs.seq_len, configs.pred_len)
        self.seasonal_predictor = nn.Linear(configs.seq_len, configs.pred_len)
        self.residual_predictor = nn.Linear(configs.seq_len, configs.pred_len)

        # è¾“å‡ºæŠ•å½±
        if configs.channel_independence == 1:
            self.projection_layer = nn.Linear(configs.d_model, 1, bias=True)
        else:
            self.projection_layer = nn.Linear(configs.d_model, configs.c_out, bias=True)

        # å¯å­¦ä¹ èåˆæƒé‡
        self.fusion_weights = nn.Parameter(torch.tensor([0.4, 0.2, 0.4]))  # [trend, seasonal, residual]

    def forward(self, x_enc, x_mark_enc, x_dec, x_mark_dec, mask=None):
        if self.task_name == 'long_term_forecast':
            return self.forecast(x_enc, x_mark_enc)
        else:
            raise ValueError('Only long_term_forecast implemented')

    def forecast(self, x_enc, x_mark_enc=None):
        B, T, N = x_enc.size()

        # å½’ä¸€åŒ–
        x_enc = self.normalize_layers[0](x_enc, 'norm')

        # ğŸ”¥ å…³é”®æ”¹è¿›ï¼šå…ˆè¿›è¡Œå…¨å±€STARä¿¡æ¯èšåˆï¼Œå†åˆ†è§£
        x_star_enhanced = self.global_star(x_enc)  # [B, T, N] - å…¨å±€ä¿¡æ¯å¢å¼º

        # åœ¨å¢å¼ºåçš„åºåˆ—ä¸Šè¿›è¡Œåˆ†è§£
        seasonal, trend = self.decomposition(x_star_enhanced)
        residual = x_star_enhanced - seasonal - trend

        # é€šé“ç‹¬ç«‹æ€§å¤„ç†
        if self.configs.channel_independence == 1:
            seasonal = seasonal.permute(0, 2, 1).contiguous().reshape(B * N, T, 1)
            trend = trend.permute(0, 2, 1).contiguous().reshape(B * N, T, 1)
            residual = residual.permute(0, 2, 1).contiguous().reshape(B * N, T, 1)

        # åµŒå…¥
        if self.configs.channel_independence == 1 and x_mark_enc is not None:
            x_mark_enc_expanded = x_mark_enc.repeat(N, 1, 1)  # [B*N, T, mark_dim]
        else:
            x_mark_enc_expanded = x_mark_enc

        seasonal_emb = self.enc_embedding(seasonal, x_mark_enc_expanded)
        trend_emb = self.enc_embedding(trend, x_mark_enc_expanded)
        residual_emb = self.enc_embedding(residual, x_mark_enc_expanded)

        # åˆ†é‡å¤„ç†ï¼ˆä¿æŒä½ çš„KANåˆ›æ–°ï¼‰
        seasonal_out = self.seasonal_processor(seasonal_emb)
        trend_out = self.trend_processor(trend_emb)
        residual_out = self.residual_processor(residual_emb)

        # æ—¶åºé¢„æµ‹
        seasonal_pred = self.seasonal_predictor(seasonal_out.permute(0, 2, 1)).permute(0, 2, 1)
        trend_pred = self.trend_predictor(trend_out.permute(0, 2, 1)).permute(0, 2, 1)
        residual_pred = self.residual_predictor(residual_out.permute(0, 2, 1)).permute(0, 2, 1)

        # æŠ•å½±
        seasonal_pred = self.projection_layer(seasonal_pred)
        trend_pred = self.projection_layer(trend_pred)
        residual_pred = self.projection_layer(residual_pred)

        # åŠ æƒèåˆ
        weights = F.softmax(self.fusion_weights, dim=0)
        dec_out = (weights[0] * trend_pred +
                   weights[1] * seasonal_pred +
                   weights[2] * residual_pred)

        # è¾“å‡ºé‡å¡‘
        if self.configs.channel_independence == 1:
            dec_out = dec_out.reshape(B, N, self.pred_len, -1)
            if dec_out.shape[-1] == 1:
                dec_out = dec_out.squeeze(-1)
            dec_out = dec_out.permute(0, 2, 1).contiguous()

        if dec_out.shape[-1] > self.configs.c_out:
            dec_out = dec_out[..., :self.configs.c_out]

        # åå½’ä¸€åŒ–
        dec_out = self.normalize_layers[0](dec_out, 'denorm')
        return dec_out